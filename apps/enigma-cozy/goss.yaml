command:
  kubespan_peers_up:
    title: Validate KubeSpan peers are up on all nodes
    exec: |
      for node in 192.168.50.10 192.168.50.11 192.168.50.12; do
        talosctl get kubespanpeerstatuses -n $node -e $node -o json 2>/dev/null | jq -se '[.[].spec.state] | all(. == "up")' || echo "false"
      done | grep -v true && echo "false" || echo "true"
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 30000
  kubespan_discovery_enabled:
    title: Validate cluster discovery is enabled
    exec: talosctl get discoveryconfig -n 192.168.50.10 -e 192.168.50.10 -o json | jq -e '.spec.discoveryEnabled'
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 10000
  clusterkeycloak_connected:
    title: Validate ClusterKeycloak is connected
    exec: kubectl get clusterkeycloak keycloak-cozy -o jsonpath='{.status.connected}'
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 10000
  helm_releases_ready:
    title: Validate all HelmRelease resources are Ready
    exec: kubectl get hr -A -o json | jq -r '.items[] | select(any(.status.conditions[]; .type=="Ready" and (.status == "True" | not))) | "\(.metadata.namespace)/\(.metadata.name)"'
    exit-status: 0
    stdout: ""
    stderr: ""
    timeout: 10000
  cozy_system_pods_healthy:
    title: Validate all pods in cozy-system namespace are Running or Succeeded
    exec: kubectl get pods -n cozy-system -o json | jq -e '[.items[] | select(.status.phase != "Running" and .status.phase != "Succeeded")] | length == 0'
    exit-status: 0
    stdout: []
    stderr: ""
    timeout: 10000
  api_certificate_san:
    title: Validate Kubernetes API certificate includes api.enigma.vgijssel.nl SAN
    exec: echo | openssl s_client -connect api.enigma.vgijssel.nl:443 -servername api.enigma.vgijssel.nl 2>/dev/null | openssl x509 -noout -text | grep -q "DNS:api.enigma.vgijssel.nl"
    exit-status: 0
    timeout: 10000
  keycloak_db_primary_has_endpoints:
    title: Validate Keycloak PostgreSQL primary service has endpoints
    exec: kubectl get endpoints keycloak-db-rw -n cozy-keycloak -o jsonpath='{.subsets[0].addresses[0].ip}' 2>/dev/null
    exit-status: 0
    stdout:
      - /^10\./
    stderr: ""
    timeout: 10000
  multus_pods_running_on_all_nodes:
    title: Validate Multus CNI pods are running on all nodes
    exec: |
      expected=$(kubectl get nodes --no-headers | wc -l)
      running=$(kubectl get pods -n cozy-multus -l app=multus --field-selector=status.phase=Running --no-headers | wc -l)
      [ "$expected" -eq "$running" ] && echo "true" || echo "false: expected $expected, got $running"
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 10000
  multus_memory_limit_sufficient:
    title: Validate Multus CNI has sufficient memory limit (>=500Mi) to handle post-reboot burst
    exec: |
      limit=$(kubectl get daemonset cozy-multus -n cozy-multus -o jsonpath='{.spec.template.spec.containers[?(@.name=="kube-multus")].resources.limits.memory}')
      # Convert to bytes for comparison (handles Mi, Gi suffixes)
      limit_bytes=$(echo "$limit" | awk '/Gi/{print $1*1024*1024*1024; next} /Mi/{print $1*1024*1024; next} /Ki/{print $1*1024; next} {print $1}' | sed 's/[^0-9]//g')
      min_bytes=$((500 * 1024 * 1024))
      if [ "$limit_bytes" -ge "$min_bytes" ]; then
        echo "true: $limit"
      else
        echo "false: $limit is below 500Mi minimum"
      fi
    exit-status: 0
    stdout:
      - /^true:/
    stderr: ""
    timeout: 10000
  kube_ovn_controller_memory_limit_sufficient:
    title: Validate kube-ovn-controller memory limit is 2Gi to prevent OOM kills
    # kube-ovn-controller with 1Gi limit gets OOM-killed under load, causing networking failures
    # for pods on affected nodes. This results in intermittent 504 errors for services like
    # coder.enigma.vgijssel.nl when ingress routes to VMs on nodes with degraded networking.
    exec: kubectl get deploy kube-ovn-controller -n cozy-kubeovn -o jsonpath='{.spec.template.spec.containers[0].resources.limits.memory}'
    exit-status: 0
    stdout:
      - 2Gi
    stderr: ""
    timeout: 30000
  cnpg_operator_running:
    title: Validate CloudNativePG operator is running
    exec: kubectl get pods -n cozy-postgres-operator -l app.kubernetes.io/name=cloudnative-pg --field-selector=status.phase=Running --no-headers | wc -l | xargs test 1 -le
    exit-status: 0
    stderr: ""
    timeout: 10000
  # Cozystack troubleshooting checklist checks
  # https://cozystack.io/docs/operations/troubleshooting/#troubleshooting-checklist
  kubernetes_nodes_ready:
    title: Validate all Kubernetes nodes are in Ready state
    exec: kubectl get nodes --no-headers | awk '{print $2}' | grep -v "^Ready$" | wc -l | xargs test 0 -eq
    exit-status: 0
    stderr: ""
    timeout: 10000
  all_pods_healthy:
    title: Validate all pods cluster-wide are Running or Completed
    exec: kubectl get pod -A --no-headers | grep -v 'Running\|Completed' | wc -l | xargs test 0 -eq
    exit-status: 0
    stderr: ""
    timeout: 30000
  linstor_nodes_online:
    title: Validate all LINSTOR nodes are Online
    exec: linstor -m node list | jq -e '[.[][] | select(.connection_status != "ONLINE")] | length == 0'
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 30000
  linstor_storage_pools_ok:
    title: Validate all LINSTOR storage pools are Ok
    exec: linstor -m storage-pool list | jq -e '[.[][] | select(.reports != null and (.reports | length > 0))] | length == 0'
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 30000
  linstor_no_faulty_resources:
    title: Validate no LINSTOR resources are in faulty state
    exec: linstor resource list --faulty
    exit-status: 0
    stdout:
      - /^\+------------------------------------------------------------------\+$/
      - /^\| ResourceName \| Node \| Layers \| Usage \| Conns \| State \| CreatedOn \|$/
      - /^\|==================================================================\|$/
    stderr: ""
    timeout: 30000
  ovn_northbound_db_healthy:
    title: Validate OVN Northbound DB cluster is healthy
    exec: ovn-appctl -t /var/run/ovn/ovnnb_db.ctl cluster/status OVN_Northbound 2>/dev/null | grep -q "Role:" && echo "healthy" || echo "unhealthy"
    exit-status: 0
    stdout:
      - healthy
    stderr: ""
    timeout: 30000
  ovn_southbound_db_healthy:
    title: Validate OVN Southbound DB cluster is healthy
    exec: ovn-appctl -t /var/run/ovn/ovnsb_db.ctl cluster/status OVN_Southbound 2>/dev/null | grep -q "Role:" && echo "healthy" || echo "unhealthy"
    exit-status: 0
    stdout:
      - healthy
    stderr: ""
    timeout: 30000
  no_cilium_kubeovn_ip_collision:
    title: Validate no IP collision between Cilium router IPs and kube-ovn pod IPs
    # IP collisions occur when Cilium assigns a router IP to cilium_host on node A,
    # but kube-ovn assigns the same IP to a pod on node B. Traffic destined for that
    # pod gets misrouted to node A (which intercepts it locally) instead of being
    # forwarded via OVN to node B, causing connection timeouts.
    exec: ./scripts/detect-cilium-kubeovn-ip-collision.sh
    exit-status: 0
    stdout: ""
    stderr: ""
    timeout: 60000
addr:
  tcp://192.168.50.50:6443:
    title: Validate Kubernetes API reachability on kubevip
    reachable: true
    timeout: 5000
  tcp://192.168.50.10:50000:
    title: Validate Talos API port reachability on illusion node
    reachable: true
    timeout: 5000
  tcp://192.168.50.11:50000:
    title: Validate Talos API port reachability on the-dome node
    reachable: true
    timeout: 5000
  tcp://192.168.50.12:50000:
    title: Validate Talos API port reachability on the-toy-factory node
    reachable: true
    timeout: 5000
http:
  https://dashboard.enigma.vgijssel.nl:
    title: Validate CozyStack dashboard is accessible
    status: 200
    timeout: 5000
  https://api.enigma.vgijssel.nl:
    title: Validate Kubernetes API is reachable
    status: 401
    allow-insecure: true
    timeout: 5000
