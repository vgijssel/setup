command:
  kubespan_peers_up:
    title: Validate KubeSpan peers are up on all nodes
    exec: |
      for node in 192.168.50.10 192.168.50.11 192.168.50.12; do
        talosctl get kubespanpeerstatuses -n $node -e $node -o json 2>/dev/null | jq -se '[.[].spec.state] | all(. == "up")' || echo "false"
      done | grep -v true && echo "false" || echo "true"
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 30000
  kubespan_lan_peers_use_direct_ips:
    title: Validate LAN Kubespan peers use direct LAN IPs (not overlay 100.64.x.x)
    # LAN nodes (192.168.50.x) should communicate via direct LAN IPs, not via Kubespan
    # overlay addresses (100.64.0.x). Using overlay for LAN traffic adds latency and
    # creates fragility during discovery service issues. See research.md for details.
    exec: ./scripts/validate-kubespan-lan-endpoints.sh
    exit-status: 0
    stdout:
      - "illusion -> the-dome: 192.168.50.11:51820 ✅"
      - "the-dome -> illusion: 192.168.50.10:51820 ✅"
      - "illusion -> the-toy-factory: 192.168.50.12:51820 ✅"
      - "the-dome -> the-toy-factory: 192.168.50.12:51820 ✅"
      - "the-toy-factory -> illusion: 192.168.50.10:51820 ✅"
      - "the-toy-factory -> the-dome: 192.168.50.11:51820 ✅"
    stderr: ""
    timeout: 60000
  kubespan_discovery_enabled:
    title: Validate cluster discovery is enabled
    exec: talosctl get discoveryconfig -n 192.168.50.10 -e 192.168.50.10 -o json | jq -e '.spec.discoveryEnabled'
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 10000
  clusterkeycloak_connected:
    title: Validate ClusterKeycloak is connected
    exec: kubectl get clusterkeycloak keycloak-cozy -o jsonpath='{.status.connected}'
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 10000
  helm_releases_ready:
    title: Validate all HelmRelease resources are Ready
    exec: kubectl get hr -A -o json | jq -r '.items[] | select(any(.status.conditions[]; .type=="Ready" and (.status == "True" | not))) | "\(.metadata.namespace)/\(.metadata.name)"'
    exit-status: 0
    stdout: ""
    stderr: ""
    timeout: 10000
  cozy_system_pods_healthy:
    title: Validate all pods in cozy-system namespace are Running or Succeeded
    exec: kubectl get pods -n cozy-system -o json | jq -e '[.items[] | select(.status.phase != "Running" and .status.phase != "Succeeded")] | length == 0'
    exit-status: 0
    stdout: []
    stderr: ""
    timeout: 10000
  api_certificate_san:
    title: Validate Kubernetes API certificate includes api.enigma.vgijssel.nl SAN
    exec: echo | openssl s_client -connect api.enigma.vgijssel.nl:443 -servername api.enigma.vgijssel.nl 2>/dev/null | openssl x509 -noout -text | grep -q "DNS:api.enigma.vgijssel.nl"
    exit-status: 0
    timeout: 10000
  keycloak_db_primary_has_endpoints:
    title: Validate Keycloak PostgreSQL primary service has endpoints
    exec: kubectl get endpoints keycloak-db-rw -n cozy-keycloak -o jsonpath='{.subsets[0].addresses[0].ip}' 2>/dev/null
    exit-status: 0
    stdout:
      - /^10\./
    stderr: ""
    timeout: 10000
  multus_pods_running_on_all_nodes:
    title: Validate Multus CNI pods are running on all nodes
    exec: |
      expected=$(kubectl get nodes --no-headers | wc -l)
      running=$(kubectl get pods -n cozy-multus -l app=multus --field-selector=status.phase=Running --no-headers | wc -l)
      [ "$expected" -eq "$running" ] && echo "true" || echo "false: expected $expected, got $running"
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 10000
  multus_memory_limit_sufficient:
    title: Validate Multus CNI has sufficient memory limit (>=1Gi) to handle post-reboot burst
    # 500Mi was insufficient when multiple CNI operations spawn concurrent processes,
    # causing daily OOM kills on illusion node (19 OOMs in 18 days). Increased to 1Gi.
    exec: |
      limit=$(kubectl get daemonset cozy-multus -n cozy-multus -o jsonpath='{.spec.template.spec.containers[?(@.name=="kube-multus")].resources.limits.memory}')
      # Convert to bytes for comparison (handles Mi, Gi suffixes)
      limit_bytes=$(echo "$limit" | awk '/Gi/{print $1*1024*1024*1024; next} /Mi/{print $1*1024*1024; next} /Ki/{print $1*1024; next} {print $1}' | sed 's/[^0-9]//g')
      min_bytes=$((1024 * 1024 * 1024))
      if [ "$limit_bytes" -ge "$min_bytes" ]; then
        echo "true: $limit"
      else
        echo "false: $limit is below 1Gi minimum"
      fi
    exit-status: 0
    stdout:
      - /^true:/
    stderr: ""
    timeout: 10000
  kube_ovn_controller_memory_limit_sufficient:
    title: Validate kube-ovn-controller memory limit is 2Gi to prevent OOM kills
    # kube-ovn-controller with 1Gi limit gets OOM-killed under load, causing networking failures
    # for pods on affected nodes. This results in intermittent 504 errors for services like
    # coder.enigma.vgijssel.nl when ingress routes to VMs on nodes with degraded networking.
    exec: kubectl get deploy kube-ovn-controller -n cozy-kubeovn -o jsonpath='{.spec.template.spec.containers[0].resources.limits.memory}'
    exit-status: 0
    stdout:
      - 2Gi
    stderr: ""
    timeout: 30000
  cnpg_operator_running:
    title: Validate CloudNativePG operator is running
    exec: kubectl get pods -n cozy-postgres-operator -l app.kubernetes.io/name=cloudnative-pg --field-selector=status.phase=Running --no-headers | wc -l | xargs test 1 -le
    exit-status: 0
    stderr: ""
    timeout: 10000
  external_dns_running:
    title: Validate external-dns deployment is ready
    exec: kubectl get deploy external-dns-external-dns -n external-dns -o jsonpath='{.status.readyReplicas}' | xargs test 1 -le
    exit-status: 0
    stderr: ""
    timeout: 10000
  tailscale_operator_running:
    title: Validate Tailscale operator deployment is ready
    exec: kubectl get deploy operator -n tailscale -o jsonpath='{.status.readyReplicas}' | xargs test 1 -le
    exit-status: 0
    stderr: ""
    timeout: 10000
  kyverno_running:
    title: Validate Kyverno admission controller is ready
    exec: kubectl get deploy kyverno-admission-controller -n kyverno -o jsonpath='{.status.readyReplicas}' | xargs test 1 -le
    exit-status: 0
    stderr: ""
    timeout: 10000
  # Cozystack troubleshooting checklist checks
  # https://cozystack.io/docs/operations/troubleshooting/#troubleshooting-checklist
  kubernetes_nodes_ready:
    title: Validate all Kubernetes nodes are in Ready state
    exec: kubectl get nodes --no-headers | awk '{print $2}' | grep -v "^Ready$" | wc -l | xargs test 0 -eq
    exit-status: 0
    stderr: ""
    timeout: 10000
  all_pods_healthy:
    title: Validate all pods cluster-wide are Running or Completed
    exec: kubectl get pod -A --no-headers | grep -v 'Running\|Completed' | wc -l | xargs test 0 -eq
    exit-status: 0
    stderr: ""
    timeout: 30000
  linstor_nodes_online:
    title: Validate all LINSTOR nodes are Online
    exec: linstor -m node list | jq -e '[.[][] | select(.connection_status != "ONLINE")] | length == 0'
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 30000
  linstor_storage_pools_ok:
    title: Validate all LINSTOR storage pools are Ok
    exec: linstor -m storage-pool list | jq -e '[.[][] | select(.reports != null and (.reports | length > 0))] | length == 0'
    exit-status: 0
    stdout:
      - "true"
    stderr: ""
    timeout: 30000
  linstor_no_faulty_resources:
    title: Validate no LINSTOR resources are in faulty state
    exec: linstor resource list --faulty
    exit-status: 0
    stdout:
      - /^\+------------------------------------------------------------------\+$/
      - /^\| ResourceName \| Node \| Layers \| Usage \| Conns \| State \| CreatedOn \|$/
      - /^\|==================================================================\|$/
    stderr: ""
    timeout: 30000
  ovn_northbound_db_healthy:
    title: Validate OVN Northbound DB cluster is healthy
    exec: ovn-appctl -t /var/run/ovn/ovnnb_db.ctl cluster/status OVN_Northbound 2>/dev/null | grep -q "Role:" && echo "healthy" || echo "unhealthy"
    exit-status: 0
    stdout:
      - healthy
    stderr: ""
    timeout: 30000
  ovn_southbound_db_healthy:
    title: Validate OVN Southbound DB cluster is healthy
    exec: ovn-appctl -t /var/run/ovn/ovnsb_db.ctl cluster/status OVN_Southbound 2>/dev/null | grep -q "Role:" && echo "healthy" || echo "unhealthy"
    exit-status: 0
    stdout:
      - healthy
    stderr: ""
    timeout: 30000
  etc_hosts_uses_management_ips:
    title: Validate /etc/hosts maps hostnames to management IPs (not br0 IPs)
    skip: true
    # After br0 removal, hostnames should resolve to management network (192.168.50.x)
    # not the old br0 network (10.50.50.x). This prevents OVN ports from binding to wrong IPs.
    exec: ./scripts/validate-etc-hosts.sh
    exit-status: 0
    stdout:
      - "illusion:"
      - 127.0.0.1     localhost
      - 192.168.50.10 illusion
      - ::1           localhost ip6-localhost ip6-loopback
      - ff02::1       ip6-allnodes
      - ff02::2       ip6-allrouters
      - 192.168.50.10 illusion
      - ""
      - "the-dome:"
      - 127.0.0.1     localhost
      - 192.168.50.11 the-dome
      - ::1           localhost ip6-localhost ip6-loopback
      - ff02::1       ip6-allnodes
      - ff02::2       ip6-allrouters
      - 192.168.50.11 the-dome
      - ""
      - "the-toy-factory:"
      - 127.0.0.1     localhost
      - 192.168.50.12 the-toy-factory
      - ::1           localhost ip6-localhost ip6-loopback
      - ff02::1       ip6-allnodes
      - ff02::2       ip6-allrouters
      - 192.168.50.12 the-toy-factory
      - ""
    stderr: ""
    timeout: 30000
  ovn_ports_bound_to_management_ips:
    title: Validate OVN database ports (6641-6644) bind to management IPs
    # OVN Northbound (6641), Southbound (6642), and RAFT cluster ports (6643, 6644)
    # must bind to management network IPs (192.168.50.x), not br0 IPs (10.50.50.x).
    # Incorrect binding causes connection refused errors and virtual cluster failures.
    exec: ./scripts/validate-ovn-port-bindings.sh
    exit-status: 0
    stdout:
      - "illusion (192.168.50.10):"
      - "  - Port 6641: 192.168.50.10 ✅"
      - "  - Port 6642: 192.168.50.10 ✅"
      - "  - Port 6643: 192.168.50.10 ✅"
      - "  - Port 6644: 192.168.50.10 ✅"
      - ""
      - "the-dome (192.168.50.11):"
      - "  - Port 6641: 192.168.50.11 ✅"
      - "  - Port 6642: 192.168.50.11 ✅"
      - "  - Port 6643: 192.168.50.11 ✅"
      - "  - Port 6644: 192.168.50.11 ✅"
      - ""
      - "the-toy-factory (192.168.50.12):"
      - "  - Port 6641: 192.168.50.12 ✅"
      - "  - Port 6642: 192.168.50.12 ✅"
      - "  - Port 6643: 192.168.50.12 ✅"
      - "  - Port 6644: 192.168.50.12 ✅"
      - ""
    stderr: ""
    timeout: 60000
  ovn_raft_cluster_uses_management_ips:
    title: Validate OVN RAFT clusters use management network IPs
    # RAFT cluster addresses in OVN database must use ssl://192.168.50.x:6643-6644
    # not ssl://10.50.50.x:6643-6644 (old br0 IPs). This ensures proper cluster communication.
    # Members are sorted by IP for stable ordering.
    exec: ./scripts/validate-ovn-raft-cluster.sh
    exit-status: 0
    stdout: |
      OVN Northbound RAFT Cluster:
        Cluster ID: 4e69 (4e69184e-18bb-480f-af60-e70a6030b1d8)
        Members:
          - 192.168.50.10 ✅
          - 192.168.50.11 ✅
          - 192.168.50.12 ✅

        OVN Southbound RAFT Cluster:
        Cluster ID: efeb (efeb3e62-6271-42dd-82a7-7640b9ebfeb3)
        Members:
          - 192.168.50.10 ✅
          - 192.168.50.11 ✅
          - 192.168.50.12 ✅
    stderr: ""
    timeout: 60000
  no_cilium_kubeovn_ip_collision:
    title: Validate no IP collision between Cilium router IPs and kube-ovn pod IPs
    # IP collisions occur when Cilium assigns a router IP to cilium_host on node A,
    # but kube-ovn assigns the same IP to a pod on node B. Traffic destined for that
    # pod gets misrouted to node A (which intercepts it locally) instead of being
    # forwarded via OVN to node B, causing connection timeouts.
    exec: ./scripts/detect-cilium-kubeovn-ip-collision.sh
    exit-status: 0
    stdout: ""
    stderr: ""
    timeout: 60000
  etcd_members_use_management_ips:
    title: Validate etcd members use management network IPs (192.168.50.x)
    # After br0 removal, etcd peer and client URLs must use management network IPs
    # (192.168.50.x), not old br0 IPs (10.50.50.x). This ensures proper cluster
    # communication and prevents 13+ second RAFT consensus latencies.
    exec: ./scripts/validate-etcd-member-ips.sh
    exit-status: 0
    stdout:
      - "etcd Member Configuration:"
      - "illusion:"
      - "  - Peer URL: https://192.168.50.10:2380 ✅"
      - "  - Client URL: https://192.168.50.10:2379 ✅"
      - ""
      - "the-dome:"
      - "  - Peer URL: https://192.168.50.11:2380 ✅"
      - "  - Client URL: https://192.168.50.11:2379 ✅"
      - ""
      - "the-toy-factory:"
      - "  - Peer URL: https://192.168.50.12:2380 ✅"
      - "  - Client URL: https://192.168.50.12:2379 ✅"
      - ""
    stderr: ""
    timeout: 30000
  etcd_service_healthy:
    title: Validate etcd service is running and healthy on all control plane nodes
    # etcd health failures cause API server unresponsiveness and 13+ second timeouts.
    # All control plane nodes must report STATE=Running and HEALTH=OK.
    exec: ./scripts/validate-etcd-health.sh
    exit-status: 0
    stdout:
      - "etcd Service Health:"
      - "illusion (192.168.50.10):"
      - "  - STATE: Running ✅"
      - "  - HEALTH: OK ✅"
      - ""
      - "the-dome (192.168.50.11):"
      - "  - STATE: Running ✅"
      - "  - HEALTH: OK ✅"
      - ""
      - "the-toy-factory (192.168.50.12):"
      - "  - STATE: Running ✅"
      - "  - HEALTH: OK ✅"
      - ""
    stderr: ""
    timeout: 30000
  talos_kernel_extensions:
    title: Validate Talos kernel extensions are installed on all nodes
    # Kernel extensions provide hardware support (GPU, network firmware) and storage
    # capabilities (ZFS, DRBD) required by the cluster. Missing extensions can cause
    # hardware failures or storage unavailability.
    exec: |
      for node in 192.168.50.10 192.168.50.11 192.168.50.12; do
        talosctl get extensions -n $node -e $node 2>&1
      done
    exit-status: 0
    stdout:
      # Expected kernel extensions on all nodes:
      - amd-ucode
      - amdgpu
      - bnx2-bnx2x
      - intel-ice-firmware
      - i915
      - intel-ucode
      - qlogic-firmware
      - drbd
      - zfs
      # Realtek firmware for network hardware support
      - realtek
    stderr: ""
    timeout: 60000
  dns_upstream_configured:
    title: Validate DNS upstream servers are configured and healthy on all nodes
    # DNS upstream configuration is critical for cluster bootstrap and ongoing resolution.
    # Control plane nodes use cluster DNS (192.168.50.2), while remote WAN nodes (here-i-am)
    # must have public DNS (1.1.1.1, 8.8.8.8) listed first to prevent bootstrap deadlock when
    # KubeSpan tunnel disconnects and needs to resolve discovery.talos.dev to re-establish.
    exec: |
      talosctl get dnsupstream --nodes 192.168.50.10,192.168.50.11,192.168.50.12,46.224.93.115 --endpoints 192.168.50.10 -o json | jq -s -r '.[] | "\(.node): \(.spec.addr) healthy=\(.spec.healthy)"'
    exit-status: 0
    stdout: |
      192.168.50.10: 192.168.50.2:53 healthy=true
      192.168.50.11: 192.168.50.2:53 healthy=true
      192.168.50.12: 192.168.50.2:53 healthy=true
      46.224.93.115: 1.1.1.1:53 healthy=true
      46.224.93.115: 8.8.8.8:53 healthy=true
    stderr: ""
    timeout: 60000
  nodes_have_no_taints:
    title: Validate no Kubernetes nodes have taints that could block DaemonSet scheduling
    # Taints can prevent critical DaemonSets (prometheus-node-exporter, fluent-bit) from
    # scheduling on nodes, causing gaps in metrics collection and logging. This regression
    # test ensures no unexpected taints are applied to nodes. If a taint is needed, it should
    # be explicitly documented and the relevant DaemonSets should have matching tolerations.
    exec: |
      kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): \(.spec.taints // [])"' | sort
    exit-status: 0
    stdout: |
      here-i-am: []
      illusion: []
      the-dome: []
      the-toy-factory: []
    stderr: ""
    timeout: 10000
  controlplane_nodes_no_loadbalancer_exclusion_label:
    title: Validate control plane nodes do not have exclude-from-external-load-balancers label
    # With allowSchedulingOnControlPlanes: true, control plane nodes should participate
    # in load balancing. Talos adds this label by default since v1.8.0, but we remove it
    # via strategic merge patch in all.patch.yaml to allow MetalLB to route traffic to them.
    exec: kubectl get nodes -l node-role.kubernetes.io/control-plane,node.kubernetes.io/exclude-from-external-load-balancers --no-headers 2>&1
    exit-status: 0
    stdout:
      - No resources found
    stderr: ""
    timeout: 10000
  storage_network_mtu_jumbo:
    title: Validate storage interfaces have MTU 9000 (jumbo frames) for 25Gbps network
    # The dedicated storage network uses MTU 9000 (jumbo frames) to reduce CPU overhead
    # and maximize throughput on the 25Gbps point-to-point links between nodes.
    # Both Mellanox interfaces (enp4s0f0np0, enp4s0f1np1) must have MTU 9000 configured.
    exec: |
      errors=0
      for node_ip in 192.168.50.10 192.168.50.11 192.168.50.12; do
        for iface in enp4s0f0np0 enp4s0f1np1; do
          mtu=$(talosctl -n $node_ip -e $node_ip read /sys/class/net/$iface/mtu 2>/dev/null | tr -d '\n ')
          if [ "$mtu" = "9000" ]; then
            echo "$node_ip:$iface MTU=$mtu ✅"
          else
            echo "$node_ip:$iface MTU=$mtu ❌ (expected 9000)"
            errors=$((errors+1))
          fi
        done
      done
      [ $errors -eq 0 ] && echo "All storage interfaces have MTU 9000" || exit 1
    exit-status: 0
    stdout:
      - "All storage interfaces have MTU 9000"
    stderr: ""
    timeout: 60000
addr:
  tcp://192.168.50.50:6443:
    title: Validate Kubernetes API reachability on kubevip
    reachable: true
    timeout: 5000
  tcp://192.168.50.10:50000:
    title: Validate Talos API port reachability on illusion node
    reachable: true
    timeout: 5000
  tcp://192.168.50.11:50000:
    title: Validate Talos API port reachability on the-dome node
    reachable: true
    timeout: 5000
  tcp://192.168.50.12:50000:
    title: Validate Talos API port reachability on the-toy-factory node
    reachable: true
    timeout: 5000
  # WireGuard/Kubespan UDP port checks
  # WireGuard uses UDP port 51820 for Kubespan mesh communication.
  # If this port is not reachable, nodes cannot establish direct peer connections.
  udp://192.168.50.10:51820:
    title: Validate WireGuard UDP port reachability on illusion node
    reachable: true
    timeout: 5000
  udp://192.168.50.11:51820:
    title: Validate WireGuard UDP port reachability on the-dome node
    reachable: true
    timeout: 5000
  udp://192.168.50.12:51820:
    title: Validate WireGuard UDP port reachability on the-toy-factory node
    reachable: true
    timeout: 5000
http:
  http://192.168.50.100:
    title: Validate MetalLB L2 advertisement is working (nginx ingress reachable via LoadBalancer IP)
    # MetalLB assigns 192.168.50.100 to the nginx ingress controller service.
    # This validates that MetalLB speakers are announcing the IP via L2/ARP
    # and traffic is being routed to the ingress pods. Returns 404 from default backend.
    status: 404
    timeout: 10000
  https://dashboard.enigma.vgijssel.nl:
    title: Validate CozyStack dashboard is accessible
    status: 200
    timeout: 5000
  https://api.enigma.vgijssel.nl:
    title: Validate Kubernetes API is reachable
    status: 401
    allow-insecure: true
    timeout: 5000
