operatorNamespace: rook-ceph

toolbox:
  # -- Enable Ceph debugging pod deployment. See [toolbox](../Troubleshooting/ceph-toolbox.md)
  enabled: false

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.2
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true
  dashboard:
    enabled: true
  network:
    provider: host
    addressRanges:
      public:
        - "10.15.15.0/24"
      cluster:
        - "10.15.15.0/24"

  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
      - name: the-toy-factory
        devices:
          - name: /dev/disk/by-id/wwn-0x5000039d78c8b66e # Toshiba 8TB
            config:
              metadataDevice: /dev/disk/by-partlabel/db-1 # Micron 2TB cache partition

          - name: /dev/disk/by-id/wwn-0x5000039d78c8c98c # Toshiba 8TB
            config:
              metadataDevice: /dev/disk/by-partlabel/db-2 # Micron 2TB cache partition

          - name: /dev/disk/by-partlabel/data # Micron 2TB data partition

      - name: the-dome
        devices:
          - name: /dev/disk/by-partlabel/data # Micron 2TB data partition

      - name: illusion
        devices:
          - name: /dev/disk/by-partlabel/data # Micron 2TB data partition

cephBlockPools: []
cephObjectStores: []
cephFileSystems:
  - name: media-filesystem
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
    spec:
      metadataPool:
        replicated:
          size: 3
        # From https://docs.ceph.com/en/quincy/cephfs/createfs/
        # use nvme / low latency devices for the metadata pool
        deviceClass: nvme
      dataPools:
        # Currently only store two copies of each file in the media file system
        # and not distributed across hosts, because only a single node has hdds currently.
        - failureDomain: osd
          replicated:
            size: 2
          # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
          name: data0
          deviceClass: hdd
      metadataServer:
        activeCount: 1
        # activeStandby: true
        # resources:
        #   limits:
        #     memory: "4Gi"
        #   requests:
        #     cpu: "1000m"
        #     memory: "4Gi"
        # priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: media-filesystem
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      annotations: {}
      labels: {}
      mountOptions: []
      # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
      parameters:
        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        # Specify the filesystem type of the volume. If not specified, csi-provisioner
        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
        # in hyperconverged settings where the volume is mounted on the same node as the osds.
        csi.storage.k8s.io/fstype: ext4

  - name: docs-filesystem
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
    spec:
      metadataPool:
        replicated:
          size: 3
        # From https://docs.ceph.com/en/quincy/cephfs/createfs/
        # use nvme / low latency devices for the metadata pool
        deviceClass: nvme
      dataPools:
        # Currently only store two copies of each file in the media file system
        # and not distributed across hosts, because only a single node has hdds currently.
        - failureDomain: host
          replicated:
            size: 3
          # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
          name: data0
          deviceClass: nvme
      metadataServer:
        activeCount: 1
        # activeStandby: true
        # resources:
        #   limits:
        #     memory: "4Gi"
        #   requests:
        #     cpu: "1000m"
        #     memory: "4Gi"
        # priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: docs-filesystem
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      annotations: {}
      labels: {}
      mountOptions: []
      # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
      parameters:
        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        # Specify the filesystem type of the volume. If not specified, csi-provisioner
        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
        # in hyperconverged settings where the volume is mounted on the same node as the osds.
        csi.storage.k8s.io/fstype: ext4
